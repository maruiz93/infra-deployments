---
# simplified Loki configuration for development
deploymentMode: Distributed

# Service Account configuration to match SCC  
serviceAccount:
  create: false
  name: vector-kubearchive-log-collector-loki

# Basic Loki configuration
loki:
  commonConfig:
    replication_factor: 2  # Match our multi-replica setup
    path_prefix: /var/loki  # This directory will be writable via volume mount
  schemaConfig:
    configs:
      - from: "2024-04-01"
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  ingester:
     chunk_encoding: snappy
  querier:
     # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
     max_concurrent: 4
  pattern_ingester:
     enabled: false
  limits_config:
       retention_period: 744h  # 31 days retention
       ingestion_rate_mb: 10
       ingestion_burst_size_mb: 20
       max_streams_per_user: 0
       max_line_size: 256000
       reject_old_samples: true
       reject_old_samples_max_age: 168h
       discover_service_name: []
       allow_structured_metadata: true
       volume_enabled: true
       max_global_streams_per_user: 50000

# Security contexts for OpenShift
podSecurityContext:
  runAsNonRoot: false
  allowPrivilegeEscalation: false

containerSecurityContext:
  runAsNonRoot: false
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true  # Keep read-only root filesystem for security

# Disable test pods
test:
  enabled: false

# Disable sidecar completely to avoid loki-sc-rules container
sidecar:
  rules:
    enabled: false
  datasources:
    enabled: false

singleBinary:
  replicas: 0

# Zero out replica counts of other deployment modes
backend:
  replicas: 0
read:
  replicas: 0
write:
  replicas: 0

# Distributed components configuration
ingester:
  replicas: 2  # Multiple replicas for realistic testing
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80
  zoneAwareReplication:
    enabled: false
  podDisruptionBudget:
    enabled: true
    minAvailable: 1  # Allow disruption of 1 pod when we have 2+
  resources:
    requests:
      cpu: 50m      # Reduced for local development
      memory: 128Mi # Reduced for local development
    limits:
      memory: 256Mi # Reduced for local development
  persistence:
    enabled: true
    size: 5Gi     # Smaller volumes for local development
    affinity: {}
  podAntiAffinity:
    # Use soft anti-affinity to allow scheduling on same node if needed
    soft:
      topologyKey: kubernetes.io/hostname
    hard: {}

querier:
  replicas: 2
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80
  podDisruptionBudget:
    enabled: true
    minAvailable: 1  # Allow disruption of 1 pod when we have 2+
  resources:
    requests:
      cpu: 50m      # Reduced for local development
      memory: 128Mi # Reduced for local development
    limits:
      memory: 256Mi # Reduced for local development
  affinity: {}
  podAntiAffinity:
    soft:
      topologyKey: kubernetes.io/hostname
    hard: {}

queryFrontend:
  replicas: 2
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  resources:
    requests:
      cpu: 25m      # Reduced for local development
      memory: 32Mi  # Reduced for local development
    limits:
      memory: 64Mi  # Reduced for local development
  podAntiAffinity:
    soft:
      topologyKey: kubernetes.io/hostname
    hard: {}

queryScheduler:
  replicas: 2
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  resources:
    requests:
      cpu: 25m      # Reduced for local development
      memory: 32Mi  # Reduced for local development
    limits:
      memory: 64Mi  # Reduced for local development
  podAntiAffinity:
    soft:
      topologyKey: kubernetes.io/hostname
    hard: {}

distributor:
  replicas: 2  # Multiple replicas for realistic testing
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 3
    targetCPUUtilizationPercentage: 80
  podDisruptionBudget:
    enabled: true
    minAvailable: 1  # Allow disruption of 1 pod when we have 2+
  resources:
    requests:
      cpu: 50m      # Reduced for local development
      memory: 128Mi # Reduced for local development
    limits:
      memory: 256Mi # Reduced for local development
  affinity: {}
  podAntiAffinity:
    # Use soft anti-affinity to allow scheduling on same node if needed
    soft:
      topologyKey: kubernetes.io/hostname
    hard: {}

compactor:
  replicas: 1
  podDisruptionBudget:
    enabled: false  # Single replica, no PDB needed
  resources:
    requests:
      cpu: 50m      # Reduced for local development
      memory: 64Mi  # Reduced for local development
    limits:
      memory: 128Mi # Reduced for local development

indexGateway:
  replicas: 2
  podDisruptionBudget:
    enabled: true
    minAvailable: 1
  resources:
    requests:
      cpu: 50m      # Reduced for local development
      memory: 128Mi # Reduced for local development
    limits:
      memory: 256Mi # Reduced for local development
  affinity: {}
  podAntiAffinity:
    soft:
      topologyKey: kubernetes.io/hostname
    hard: {}

ruler:
  replicas: 1
  podDisruptionBudget:
    enabled: false  # Single replica, no PDB needed
  resources:
    requests:
      cpu: 25m      # Reduced for local development
      memory: 64Mi  # Reduced for local development
    limits:
      memory: 128Mi # Reduced for local development

bloomPlanner:
  replicas: 0
bloomBuilder:
  replicas: 0
bloomGateway:
  replicas: 0

 # This exposes the Loki gateway so it can be written to and queried externaly

gateway:
  enabled: true
  replicas: 1
  podDisruptionBudget:
    enabled: false  # Single replica, no PDB needed
  image:
    registry: docker.io
    repository: nginxinc/nginx-unprivileged
    tag: 1.24-alpine
  resources:
    requests:
      cpu: 25m      # Reduced for local development
      memory: 32Mi  # Reduced for local development
    limits:
      memory: 64Mi  # Reduced for local development
  service:
    type: ClusterIP
    port: 80
  nginxConfig:
    resolver: "dns-default.openshift-dns.svc.cluster.local."

# Enable lokiCanary for health checks
lokiCanary:
  enabled: true
  podDisruptionBudget:
    enabled: false  # Single replica, no PDB needed
  resources:
    requests:
      cpu: 25m      # Reduced for local development
      memory: 32Mi  # Reduced for local development
    limits:
      memory: 64Mi  # Reduced for local development

# Enable minio for storage
minio:
  enabled: true
  rootUser: loki
  rootPassword: supersecret
  mode: standalone
  persistence:
    enabled: true
    size: 10Gi
  resources:
    requests:
      cpu: 50m       # Reduced for local development
      memory: 64Mi   # Reduced for local development
    limits:
      memory: 128Mi  # Reduced for local development
  postJob:
    enabled: false
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        memory: 128Mi
  # OpenShift security context settings
  securityContext:
    enabled: false
    runAsUser: ""
    runAsGroup: ""
    fsGroup: ""
  containerSecurityContext:
    enabled: false
    runAsUser: ""
    runAsGroup: ""
    allowPrivilegeEscalation: false
    capabilities:
      drop:
        - ALL
    readOnlyRootFilesystem: false
  podSecurityContext:
    enabled: false
    runAsUser: ""
    runAsGroup: ""
    fsGroup: ""

# Resources for memcached exporter to satisfy linter
memcachedExporter:
  enabled: true
  resources:
    requests:
      cpu: 25m      # Reduced for local development
      memory: 32Mi  # Reduced for local development
    limits:
      memory: 64Mi  # Reduced for local development